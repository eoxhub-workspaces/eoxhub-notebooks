{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# CDSE Tiled Data Access\n",
    "\n",
    "This notebook shows how to access data from CDSE programatically, converting it to a possible wanted format (such as Cloud Optimized Geotiff - COG) and lastly registers it into an eoAPI instance.\n",
    "\n",
    "Following steps are addressed:\n",
    "* **Searches** for scenes (skipping those already in eoAPI).\n",
    "* **Downloads** tiles in parallel.\n",
    "* **Converts** to COG (skipping if file exists).\n",
    "* **Registers** in eoAPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we make sure we import all used dependencies\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import math\n",
    "import threading\n",
    "import concurrent.futures\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import requests\n",
    "from oauthlib.oauth2 import BackendApplicationClient\n",
    "from oauthlib.oauth2.rfc6749.errors import TokenExpiredError\n",
    "from requests_oauthlib import OAuth2Session\n",
    "\n",
    "import rasterio\n",
    "from rasterio.io import MemoryFile\n",
    "from rasterio.merge import merge\n",
    "from rasterio.enums import Resampling\n",
    "from rio_cogeo.cogeo import cog_translate\n",
    "from rio_cogeo.profiles import cog_profiles\n",
    "import pystac\n",
    "\n",
    "from sentinelhub import SHConfig, BBox, bbox_to_dimensions, CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General configuration\n",
    "\n",
    "# Output directory\n",
    "output_dir = Path(\"./bucket/samples/sentinel2_cogs\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# eoAPI Configuration\n",
    "EOAPI_URL = \"http://eoapi-rw-stac:8080\"\n",
    "STAC_COLLECTION_ID = \"sentinel-2-vienna-cdse\"\n",
    "S3_BUCKET = f\"s3://{os.getenv(\"workspace_BUCKET\")}\"\n",
    "S3_PREFIX = \"sentinel2_cogs\"\n",
    "\n",
    "ALL_BANDS = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n",
    "\n",
    "# Area of Interest\n",
    "bbox = BBox(bbox=[16.15, 47.95, 16.55, 48.3], crs=CRS.WGS84)\n",
    "end_date = datetime.now()\n",
    "# Trying to take most recent scenes\n",
    "start_date = end_date - timedelta(days=360)\n",
    "\n",
    "# Please make sure you the cdse secret is available in\n",
    "# the Credentials Manager and injection is enabled\n",
    "# or replace this with you own CDSE tokens\n",
    "CLIENT_ID = os.getenv(\"cdse_CLIENT_ID\")\n",
    "CLIENT_SECRET = os.getenv(\"cdse_CLIENT_SECRET\")\n",
    "TOKEN_URL = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n",
    "SH_BASE_URL = \"https://sh.dataspace.copernicus.eu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authentication Manager\n",
    "This class wraps the OAuth session to handle token refreshing automatically as it is possible to loose the session while accessing the various tiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CDSESessionManager:\n",
    "    \"\"\"\n",
    "    Thread-safe session manager that automatically refreshes the token \n",
    "    when a TokenExpiredError occurs.\n",
    "    \"\"\"\n",
    "    def __init__(self, client_id, client_secret):\n",
    "        self.client_id = client_id\n",
    "        self.client_secret = client_secret\n",
    "        self.token_url = TOKEN_URL\n",
    "        self.lock = threading.Lock()\n",
    "        self.session = None\n",
    "        self.authenticate()\n",
    "\n",
    "    def authenticate(self):\n",
    "        \"\"\"Create a new authenticated session\"\"\"\n",
    "        print(\"Authenticating with CDSE...\")\n",
    "        client = BackendApplicationClient(client_id=self.client_id)\n",
    "        self.session = OAuth2Session(client=client)\n",
    "        token = self.session.fetch_token(\n",
    "            token_url=self.token_url,\n",
    "            client_secret=self.client_secret,\n",
    "            include_client_id=True\n",
    "        )\n",
    "        print(f\"   Token obtained. Expires in {token.get('expires_in', 'unknown')}s\")\n",
    "        return self.session\n",
    "\n",
    "    def refresh_token(self):\n",
    "        \"\"\"Thread-safe token refresh\"\"\"\n",
    "        with self.lock:\n",
    "            # Check if token was just refreshed by another thread to avoid double-refresh\n",
    "            # (Simple heuristic: just re-auth)\n",
    "            print(\"\\nToken expired. Refreshing...\", end=\" \")\n",
    "            try:\n",
    "                self.authenticate()\n",
    "                print(\"Done.\")\n",
    "            except Exception as e:\n",
    "                print(f\"FAILED: {e}\")\n",
    "                raise\n",
    "\n",
    "    def post(self, url, json_data, retries=1):\n",
    "        \"\"\"Wrapper for session.post with auto-refresh logic\"\"\"\n",
    "        try:\n",
    "            response = self.session.post(url, json=json_data)\n",
    "            # Sometimes the server returns 401 instead of the library raising TokenExpiredError\n",
    "            if response.status_code == 401:\n",
    "                raise TokenExpiredError(\"401 Unauthorized\")\n",
    "            response.raise_for_status()\n",
    "            return response\n",
    "            \n",
    "        except TokenExpiredError:\n",
    "            if retries > 0:\n",
    "                self.refresh_token()\n",
    "                # Retry the request with new session\n",
    "                return self.post(url, json_data, retries=retries-1)\n",
    "            else:\n",
    "                raise\n",
    "\n",
    "# Initialize the Manager\n",
    "auth_manager = CDSESessionManager(CLIENT_ID, CLIENT_SECRET)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection Setup\n",
    "coll_payload = {\n",
    "    \"type\": \"Collection\", \"stac_version\": \"1.0.0\", \"id\": STAC_COLLECTION_ID,\n",
    "    \"title\": \"Vienna Sentinel-2\", \"description\": \"Imported from CDSE\",\n",
    "    \"eodash:rasterform\": \"https://workspace-ui-public.gtif-austria.hub-otc.eox.at/api/public/share/public-4wazei3y-02/test/aducat_s2_form.json\",\n",
    "    \"license\": \"CC-BY-4.0\",\n",
    "    \"extent\": {\"spatial\": {\"bbox\": [[16.2, 48.1, 16.6, 48.3]]}, \"temporal\": {\"interval\": [[\"2020-01-01T00:00:00Z\", None]]}},\n",
    "    \"links\": []\n",
    "}\n",
    "try: \n",
    "    requests.post(f\"{EOAPI_URL}/collections\", json=coll_payload)\n",
    "except: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search in CDSE STAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Search for Latest Sentinel-2 Scenes\n",
    "# --------------------------------------------------------------------------------\n",
    "\n",
    "def get_existing_dates(eoapi_url, collection_id):\n",
    "    \"\"\"\n",
    "    Fetch all unique dates (YYYY-MM-DD) currently registered in the STAC collection.\n",
    "    \"\"\"\n",
    "    existing_dates = set()\n",
    "    url = f\"{eoapi_url}/collections/{collection_id}/items\"\n",
    "    \n",
    "    # Simple pagination handling to get all items\n",
    "    params = {\"limit\": 100} \n",
    "    \n",
    "    try:\n",
    "        print(f\" Checking existing dates in collection '{collection_id}'...\")\n",
    "        while url:\n",
    "            response = requests.get(url, params=params)\n",
    "            if response.status_code == 404:\n",
    "                return set() # Collection likely doesn't exist yet\n",
    "            \n",
    "            response.raise_for_status()\n",
    "            data = response.json()\n",
    "            \n",
    "            for feature in data.get('features', []):\n",
    "                # Extract YYYY-MM-DD\n",
    "                date_str = feature['properties']['datetime'][:10]\n",
    "                existing_dates.add(date_str)\n",
    "                \n",
    "            # Check for next page link\n",
    "            url = None\n",
    "            links = data.get('links', [])\n",
    "            for link in links:\n",
    "                if link['rel'] == 'next':\n",
    "                    url = link['href']\n",
    "                    params = {} # Params usually included in next link\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(f\" Could not fetch existing items (starting fresh?): {e}\")\n",
    "        \n",
    "    return existing_dates\n",
    "\n",
    "def search_sentinel2_scenes(auth_manager, bbox, start_date, end_date, max_results=2):\n",
    "    \"\"\"\n",
    "    Search for Sentinel-2 scenes using CDSE OpenSearch API,\n",
    "    filtering out days that are already in the STAC collection.\n",
    "    \"\"\"\n",
    "    base_url = \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json\"\n",
    "    \n",
    "    params = {\n",
    "        \"box\": f\"{bbox.min_x},{bbox.min_y},{bbox.max_x},{bbox.max_y}\",\n",
    "        \"startDate\": start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"completionDate\": end_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "        \"processingLevel\": \"S2MSI2A\", \n",
    "        \"cloudCover\": \"[0,5]\",\n",
    "        \"maxRecords\": 200,\n",
    "        \"sortParam\": \"startDate\",\n",
    "        \"sortOrder\": \"descending\"\n",
    "    }\n",
    "    \n",
    "    # 1. Fetch existing dates from your catalog\n",
    "    existing_dates = get_existing_dates(EOAPI_URL, STAC_COLLECTION_ID)\n",
    "    if existing_dates:\n",
    "        print(f\" Found {len(existing_dates)} days already registered. Skipping these dates.\")\n",
    "    \n",
    "    # 2. Search CDSE\n",
    "    print(\" Searching CDSE catalog...\")\n",
    "    response = requests.get(base_url, params=params)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    results = response.json()\n",
    "    features = results.get('features', [])\n",
    "    \n",
    "    # 3. Filter results\n",
    "    # Initialize seen_dates with what is already in the DB so we don't pick them again\n",
    "    seen_dates = existing_dates.copy()\n",
    "    converted_features = []\n",
    "    \n",
    "    for feature in features:\n",
    "        scene_id = feature['id']\n",
    "        # Get YYYY-MM-DD\n",
    "        scene_date = feature['properties']['startDate'][:10]\n",
    "        \n",
    "        # SKIP IF DATE ALREADY EXISTS (in DB or earlier in this loop)\n",
    "        if scene_date in seen_dates:\n",
    "            continue\n",
    "            \n",
    "        # Mark this date as seen so we don't pick another scene from the same day\n",
    "        seen_dates.add(scene_date)\n",
    "        \n",
    "        converted_features.append({\n",
    "            'id': scene_id,\n",
    "            'properties': {\n",
    "                'datetime': feature['properties']['startDate'],\n",
    "                'eo:cloud_cover': feature['properties'].get('cloudCover', 0)\n",
    "            },\n",
    "            'geometry': feature.get('geometry'),\n",
    "            'bbox': feature.get('bbox')\n",
    "        })\n",
    "        \n",
    "        if len(converted_features) >= max_results:\n",
    "            break\n",
    "    \n",
    "    return converted_features\n",
    "\n",
    "# Run the search, here we configure the area and time of interest as well as how many scenes to fetch\n",
    "scenes = search_sentinel2_scenes(auth_manager, bbox, start_date, end_date, 5)\n",
    "\n",
    "print(f\"\\nSelected {len(scenes)} NEW scenes for processing:\")\n",
    "for i, scene in enumerate(scenes, 1):\n",
    "    print(f\"{i}. ID: {scene['id']}\")\n",
    "    print(f\"   Date: {scene['properties']['datetime']}\")\n",
    "    print(f\"   Cloud Cover: {scene['properties'].get('eo:cloud_cover', 'N/A')}%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Configuration ---\n",
    "TARGET_RES = 10 \n",
    "MAX_PIXELS = 2000 \n",
    "MAX_WORKERS = 2 \n",
    "MAX_RETRIES = 3\n",
    "RETRY_DELAY = 2\n",
    "\n",
    "@dataclass\n",
    "class SimpleBBox:\n",
    "    min_x: float; min_y: float; max_x: float; max_y: float\n",
    "\n",
    "def split_bbox_into_tiles(bbox):\n",
    "    w_deg, h_deg = bbox.max_x - bbox.min_x, bbox.max_y - bbox.min_y\n",
    "    center_lat = (bbox.min_y + bbox.max_y) / 2\n",
    "    m_lat, m_lon = 111320, 111320 * math.cos(math.radians(center_lat))\n",
    "    tot_w_px, tot_h_px = int((w_deg * m_lon) / TARGET_RES), int((h_deg * m_lat) / TARGET_RES)\n",
    "    cols, rows = math.ceil(tot_w_px / MAX_PIXELS), math.ceil(tot_h_px / MAX_PIXELS)\n",
    "    \n",
    "    tiles = []\n",
    "    print(f\"    Tiling: {cols}x{rows} grid\")\n",
    "    for r in range(rows):\n",
    "        for c in range(cols):\n",
    "            tiles.append({\n",
    "                \"bbox\": SimpleBBox(\n",
    "                    bbox.min_x + (c * (w_deg/cols)), \n",
    "                    bbox.min_y + (r * (h_deg/rows)), \n",
    "                    bbox.min_x + ((c+1) * (w_deg/cols)), \n",
    "                    bbox.min_y + ((r+1) * (h_deg/rows))\n",
    "                ),\n",
    "                \"size\": (int(tot_w_px/cols), int(tot_h_px/rows)),\n",
    "                \"index\": (r, c)\n",
    "            })\n",
    "    return tiles\n",
    "\n",
    "def convert_to_cog(path, arr, transform, crs):\n",
    "    prof = cog_profiles.get(\"deflate\").copy()\n",
    "    prof.update({\n",
    "        \"BIGTIFF\": \"IF_NEEDED\",\n",
    "        \"blockxsize\": 512,\n",
    "        \"blockysize\": 512,\n",
    "        \"photometric\": \"MINISBLACK\"\n",
    "    })\n",
    "    with MemoryFile() as mem:\n",
    "        with mem.open(driver=\"GTiff\", dtype=str(arr.dtype), count=arr.shape[0], \n",
    "                      height=arr.shape[1], width=arr.shape[2], crs=crs, \n",
    "                      transform=transform, photometric=\"MINISBLACK\") as dst:\n",
    "            dst.write(arr)\n",
    "            dst.descriptions = tuple(ALL_BANDS)\n",
    "            dst.build_overviews([2,4,8,16], Resampling.nearest)\n",
    "            dst.update_tags(ns='rio_overview', resampling='nearest')\n",
    "        with mem.open() as src:\n",
    "            cog_translate(src, path, prof, in_memory=True, quiet=True)\n",
    "    print(f\"   COG Saved: {path}\")\n",
    "\n",
    "def download_single_tile(auth_mgr, tile_data, scene_id, scene_datetime):\n",
    "    \"\"\"Worker function with RETRY logic\"\"\"\n",
    "    dt = datetime.fromisoformat(scene_datetime.replace('Z', '+00:00'))\n",
    "    \n",
    "    # ... (Evalscript needed for data extraction) ...\n",
    "    evalscript = \"\"\"\n",
    "    //VERSION=3\n",
    "    function setup() {\n",
    "        return {\n",
    "            input: [{ bands: \"\"\" + json.dumps(ALL_BANDS) + \"\"\", units: \"DN\" }],\n",
    "            output: { bands: \"\"\" + str(len(ALL_BANDS)) + \"\"\", sampleType: \"UINT16\" }\n",
    "        };\n",
    "    }\n",
    "    function evaluatePixel(sample) {\n",
    "        return [\"\"\" + \", \".join([f\"sample.{b}\" for b in ALL_BANDS]) + \"\"\"];\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    t_bbox = tile_data['bbox']\n",
    "    payload = {\n",
    "        \"input\": {\n",
    "            \"bounds\": {\n",
    "                \"bbox\": [t_bbox.min_x, t_bbox.min_y, t_bbox.max_x, t_bbox.max_y],\n",
    "                \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"}\n",
    "            },\n",
    "            \"data\": [{\n",
    "                \"type\": \"sentinel-2-l2a\",\n",
    "                \"dataFilter\": {\n",
    "                    \"timeRange\": {\n",
    "                        \"from\": (dt - timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n",
    "                        \"to\": (dt + timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "                    }\n",
    "                }\n",
    "            }]\n",
    "        },\n",
    "        \"output\": {\n",
    "            \"width\": tile_data['size'][0], \"height\": tile_data['size'][1],\n",
    "            \"responses\": [{\"identifier\": \"default\", \"format\": {\"type\": \"image/tiff\"}}]\n",
    "        },\n",
    "        \"evalscript\": evalscript\n",
    "    }\n",
    "    \n",
    "    # --- Retry Loop ---\n",
    "    for attempt in range(1, MAX_RETRIES + 1):\n",
    "        try:\n",
    "            response = auth_mgr.post(f\"{SH_BASE_URL}/api/v1/process\", payload)\n",
    "            # Assuming auth_mgr raises an exception on non-200. \n",
    "            # If not, check response.status_code here.\n",
    "            \n",
    "            # Simple validation to ensure we actually got data\n",
    "            if response.content and len(response.content) > 100:\n",
    "                return tile_data['index'], response.content\n",
    "            else:\n",
    "                raise Exception(\"Empty response received\")\n",
    "\n",
    "        except Exception as e:\n",
    "            # If this was the last attempt, fail\n",
    "            if attempt == MAX_RETRIES:\n",
    "                print(f\"      Tile {tile_data['index']} failed permanently: {e}\")\n",
    "                return tile_data['index'], None\n",
    "            \n",
    "            # Otherwise, wait and retry\n",
    "            sleep_time = RETRY_DELAY * attempt\n",
    "            print(f\"      Tile {tile_data['index']} failed (Attempt {attempt}/{MAX_RETRIES}). Retrying in {sleep_time}s...\")\n",
    "            time.sleep(sleep_time)\n",
    "\n",
    "    return tile_data['index'], None\n",
    "\n",
    "# --- Main Loop ---\n",
    "processed_scenes = []\n",
    "tiles = split_bbox_into_tiles(bbox) \n",
    "\n",
    "for scene in scenes:\n",
    "    s_id, s_dt = scene['id'], scene['properties']['datetime']\n",
    "    cog_path = output_dir / f\"{s_dt[:10]}_{s_id}_cog.tif\"\n",
    "    print(f\"\\nProcessing {s_id}...\")\n",
    "\n",
    "    if cog_path.exists():\n",
    "        print(f\"   File exists. Skipping download.\")\n",
    "        processed_scenes.append({\"id\": s_id, \"path\": cog_path, \"props\": scene['properties'], \"geom\": scene['geometry']})\n",
    "        continue\n",
    "\n",
    "    print(f\"   Downloading {len(tiles)} tiles (Parallel)...\")\n",
    "    tile_results = {}\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        futures = {executor.submit(download_single_tile, auth_manager, t, s_id, s_dt): t for t in tiles}\n",
    "        \n",
    "        for f in concurrent.futures.as_completed(futures):\n",
    "            idx, content = f.result()\n",
    "            # Only store if content is valid\n",
    "            if content: \n",
    "                tile_results[idx] = content\n",
    "    \n",
    "    # --- Completeness Check ---\n",
    "    # We compare the number of valid results to the number of expected tiles.\n",
    "    if len(tile_results) != len(tiles):\n",
    "        missing_count = len(tiles) - len(tile_results)\n",
    "        print(f\"   Scene failed. Missing {missing_count} tiles. Discarding scene.\")\n",
    "        # We 'continue' here to move to the next scene without stitching\n",
    "        continue\n",
    "\n",
    "    print(\"   Stitching & Converting...\")\n",
    "    m_files, m_dsets = [], []\n",
    "    try:\n",
    "        for k in sorted(tile_results.keys()):\n",
    "            mf = MemoryFile(tile_results[k])\n",
    "            m_files.append(mf); m_dsets.append(mf.open())\n",
    "        \n",
    "        mosaic, trans = merge(m_dsets)\n",
    "        convert_to_cog(str(cog_path), mosaic, trans, m_dsets[0].crs)\n",
    "        \n",
    "        if cog_path.exists():\n",
    "            processed_scenes.append({\"id\": s_id, \"path\": cog_path, \"props\": scene['properties'], \"geom\": scene['geometry']})\n",
    "    except Exception as e:\n",
    "        print(f\"   Stitching failed: {e}\")\n",
    "    finally:\n",
    "        for d in m_dsets: d.close()\n",
    "        for m in m_files: m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register in eoAPI\n",
    "\n",
    "If wanted this example continues explaining how the retrieved data can be registered into an eoAPI instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_register(scene, eoapi_url, collection_id):\n",
    "    s_id, props = scene['id'], scene['props']\n",
    "    dt = datetime.fromisoformat(props['datetime'].replace('Z', '+00:00'))\n",
    "    cog_url = f\"{S3_BUCKET}/{S3_PREFIX}/{scene['path'].name}\"\n",
    "    \n",
    "    # Geometry fallback\n",
    "    geom = scene['geom']\n",
    "    if not geom: \n",
    "        geom = {\"type\": \"Polygon\", \"coordinates\": [[ [bbox.min_x, bbox.min_y], [bbox.max_x, bbox.min_y], [bbox.max_x, bbox.max_y], [bbox.min_x, bbox.max_y], [bbox.min_x, bbox.min_y] ]]}\n",
    "        \n",
    "    # Get BBox from geometry for STAC Item\n",
    "    coords = geom['coordinates'][0]\n",
    "    xs, ys = [c[0] for c in coords], [c[1] for c in coords]\n",
    "    ibbox = [min(xs), min(ys), max(xs), max(ys)]\n",
    "\n",
    "    item = pystac.Item(id=s_id, geometry=geom, bbox=ibbox, datetime=dt, \n",
    "                       properties={\"eo:cloud_cover\": props.get('eo:cloud_cover', 0)}, collection=collection_id)\n",
    "    \n",
    "    item.add_asset(\"cog\", pystac.Asset(href=cog_url, media_type=\"image/tiff; application=geotiff; profile=cloud-optimized\", roles=[\"data\", \"visual\"]))\n",
    "    \n",
    "    # Add XYZ Preview Link\n",
    "    eoapi_endpoint = os.getenv(\"workspace_RASTER_ENDPOINT\")\n",
    "    xyz = f\"{eoapi_endpoint}/collections/{collection_id}/items/{s_id}/tiles/WebMercatorQuad/{{z}}/{{x}}/{{y}}@1x?assets=cog&asset_bidx=cog|4,3,2&rescale=0,3000\"\n",
    "    item.add_link(pystac.Link(rel=\"xyz\", target=xyz, media_type=\"application/json\"))\n",
    "\n",
    "    res = requests.post(f\"{eoapi_url}/collections/{collection_id}/items\", json=item.to_dict())\n",
    "    if res.status_code in [200, 201]: print(f\" Registered {s_id}\")\n",
    "    else: print(f\" Failed {s_id}: {res.text}\")\n",
    "\n",
    "print(f\"Registration...\")\n",
    "for s in processed_scenes:\n",
    "    create_and_register(s, EOAPI_URL, STAC_COLLECTION_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------------------\n",
    "# Finally we update Collection Temporal Extent\n",
    "# --------------------------------------------------------------------------------\n",
    "import requests\n",
    "\n",
    "def update_collection_extent(eoapi_url, collection_id):\n",
    "    print(f\"Updating temporal extent for collection: '{collection_id}'...\")\n",
    "\n",
    "    # 1. Fetch all item datetimes to determine the full range\n",
    "    all_datetimes = []\n",
    "    items_url = f\"{eoapi_url}/collections/{collection_id}/items\"\n",
    "    params = {\"limit\": 100}\n",
    "\n",
    "    try:\n",
    "        # Loop through pages to get every single item's date\n",
    "        while items_url:\n",
    "            r = requests.get(items_url, params=params)\n",
    "            r.raise_for_status()\n",
    "            data = r.json()\n",
    "            \n",
    "            for feature in data.get('features', []):\n",
    "                all_datetimes.append(feature['properties']['datetime'])\n",
    "            \n",
    "            # Handle Pagination (Follow 'next' link)\n",
    "            items_url = None\n",
    "            for link in data.get('links', []):\n",
    "                if link['rel'] == 'next':\n",
    "                    items_url = link['href']\n",
    "                    params = {} # Parameters are usually included in the 'next' href\n",
    "                    break\n",
    "\n",
    "        if not all_datetimes:\n",
    "            print(\" No items found in collection. Skipping extent update.\")\n",
    "            return\n",
    "\n",
    "        # 2. Calculate new Min/Max\n",
    "        min_dt = min(all_datetimes)\n",
    "        max_dt = max(all_datetimes)\n",
    "        \n",
    "        print(f\"   Found {len(all_datetimes)} items.\")\n",
    "        print(f\"   Calculated Interval: {min_dt}  <-->  {max_dt}\")\n",
    "\n",
    "        # 3. Fetch the current Collection definition\n",
    "        coll_url = f\"{eoapi_url}/collections/{collection_id}\"\n",
    "        coll_resp = requests.get(coll_url)\n",
    "        coll_resp.raise_for_status()\n",
    "        coll_data = coll_resp.json()\n",
    "        \n",
    "        # 4. Update the extent in the JSON\n",
    "        # STAC Spec: extent.temporal.interval is a list of lists [[start, end]]\n",
    "        coll_data['extent']['temporal']['interval'] = [[min_dt, max_dt]]\n",
    "        \n",
    "        # 5. Send PUT request to save changes\n",
    "        update_resp = requests.put(coll_url, json=coll_data)\n",
    "        \n",
    "        if update_resp.status_code in [200, 204]:\n",
    "            print(f\" Success! Collection '{collection_id}' temporal extent updated.\")\n",
    "        else:\n",
    "            print(f\" Failed to update collection. Server returned: {update_resp.status_code}\")\n",
    "            print(update_resp.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error updating extent: {e}\")\n",
    "\n",
    "# Run the update\n",
    "update_collection_extent(EOAPI_URL, STAC_COLLECTION_ID)"
   ]
  }
 ],
 "metadata": {
  "frontmatter": {
    "title": "CDSE Tiled Data Access",
    "description": "This notebook shows how to access data from CDSE programatically, converting it to a possible wanted format (such as Cloud Optimized Geotiff - COG) and lastly registers it into an eoAPI instance."
  },
  "kernelspec": {
   "display_name": "global-global-eoxhub_general",
   "language": "python",
   "name": "conda-env-global-global-eoxhub_general-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
