{"version":"1","records":[{"hierarchy":{"lvl1":"EOxHub Workspaces notebooks"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"EOxHub Workspaces notebooks"},"content":"A curated list of Jupyter notebooks demonstrating the capabilities of the\nEOxHub workspaces. These notebooks can be directly executed by\nEOxHub workspace customers within their own Jupyter Lab environment without\nfurther setup.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"CDSE Basic Data Access"},"type":"lvl1","url":"/notebooks/data-access/cdse-basic-data-access","position":0},{"hierarchy":{"lvl1":"CDSE Basic Data Access"},"content":"This notebook demonstrates the simplest way to authenticate with the Copernicus Data Space Ecosystem (CDSE), download a Sentinel-2 image, and render it.\n\nimport os\nimport io\nimport requests\nimport rasterio\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom requests_oauthlib import OAuth2Session\n\n\n\n# AUTHENTICATION\n# Replace with your actual credentials or ensure environment variables are set\n# Please make sure to not share a notebook with filled credentials!\nCLIENT_ID = os.getenv(\"cdse_CLIENT_ID\", \"your_id_here\")\nCLIENT_SECRET = os.getenv(\"cdse_CLIENT_SECRET\", \"your_secret_here\")\nTOKEN_URL = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\n\nclient = BackendApplicationClient(client_id=CLIENT_ID)\noauth = OAuth2Session(client=client)\ntoken = oauth.fetch_token(token_url=TOKEN_URL, client_secret=CLIENT_SECRET, include_client_id=True)\nprint(\"Authenticated successfully.\")\n\n\n\nfrom datetime import datetime, timedelta\n\n# 1. Parse the found date and create a time window\n# We remove the trailing 'Z' to make it compatible with fromisoformat\ndt_str = found_date.replace('Z', '')\ndt = datetime.fromisoformat(dt_str)\n\n# Create a 1-hour window around the scene\ntime_from = (dt - timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\ntime_to = (dt + timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n\nprint(f\"Requesting time window: {time_from} to {time_to}\")\n\n# 2. Updated Payload with buffer\nevalscript = \"\"\"\n//VERSION=3\nfunction setup() {\n    return {\n        input: [{ bands: [\"B04\", \"B03\", \"B02\"], units: \"DN\" }],\n        output: { bands: 3, sampleType: \"UINT16\" }\n    };\n}\nfunction evaluatePixel(sample) {\n    return [sample.B04, sample.B03, sample.B02];\n}\n\"\"\"\n\npayload = {\n    \"input\": {\n        \"bounds\": {\n            \"bbox\": [16.3, 48.15, 16.4, 48.25],\n            \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"}\n        },\n        \"data\": [{\n            \"type\": \"sentinel-2-l2a\",\n            \"dataFilter\": {\n                \"timeRange\": {\n                    \"from\": time_from,\n                    \"to\": time_to\n                }\n            }\n        }]\n    },\n    \"output\": {\n        \"width\": 512, \n        \"height\": 512,\n        \"responses\": [{\"identifier\": \"default\", \"format\": {\"type\": \"image/tiff\"}}]\n    },\n    \"evalscript\": evalscript\n}\n\nresponse = oauth.post(\"https://sh.dataspace.copernicus.eu/api/v1/process\", json=payload)\nresponse.raise_for_status()\nprint(\"Data retrieved successfully!\")\n\n\n\nimport numpy as np\n\nwith rasterio.open(io.BytesIO(response.content)) as src:\n    img = src.read().transpose(1, 2, 0)\n\n# Check if we actually have data\nprint(f\"Max pixel value: {img.max()}\")\nprint(f\"Mean pixel value: {img.mean()}\")\n\n# Scale: 3000 is a good baseline for S2; increase to 2000 for even brighter images\nimg_display = np.clip(img / 3000, 0, 1)\n\nplt.figure(figsize=(5, 5))\nplt.imshow(img_display)\nplt.axis('off')\nplt.show()\n\n","type":"content","url":"/notebooks/data-access/cdse-basic-data-access","position":1},{"hierarchy":{"lvl1":"CDSE Tiled Data Access"},"type":"lvl1","url":"/notebooks/data-access/cdse-tiled-data-access","position":0},{"hierarchy":{"lvl1":"CDSE Tiled Data Access"},"content":"This notebook shows how to access data from CDSE programatically, converting it to a possible wanted format (such as Cloud Optimized Geotiff - COG) and lastly registers it into an eoAPI instance.\n\nFollowing steps are addressed:\n\nSearches for scenes (skipping those already in eoAPI).\n\nDownloads tiles in parallel.\n\nConverts to COG (skipping if file exists).\n\nRegisters in eoAPI.\n\n# First we make sure we import all used dependencies\nimport os\nimport time\nimport json\nimport math\nimport threading\nimport concurrent.futures\nfrom datetime import datetime, timedelta\nfrom pathlib import Path\nfrom dataclasses import dataclass\n\nimport requests\nfrom oauthlib.oauth2 import BackendApplicationClient\nfrom oauthlib.oauth2.rfc6749.errors import TokenExpiredError\nfrom requests_oauthlib import OAuth2Session\n\nimport rasterio\nfrom rasterio.io import MemoryFile\nfrom rasterio.merge import merge\nfrom rasterio.enums import Resampling\nfrom rio_cogeo.cogeo import cog_translate\nfrom rio_cogeo.profiles import cog_profiles\nimport pystac\n\nfrom sentinelhub import SHConfig, BBox, bbox_to_dimensions, CRS\n\n\n\n# General configuration\n\n# Output directory\noutput_dir = Path(\"./bucket/samples/sentinel2_cogs\")\noutput_dir.mkdir(exist_ok=True)\n\n# eoAPI Configuration\nEOAPI_URL = \"http://eoapi-rw-stac:8080\"\nSTAC_COLLECTION_ID = \"sentinel-2-vienna-cdse\"\nS3_BUCKET = f\"s3://{os.getenv(\"workspace_BUCKET\")}\"\nS3_PREFIX = \"sentinel2_cogs\"\n\nALL_BANDS = [\"B01\", \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B11\", \"B12\"]\n\n# Area of Interest\nbbox = BBox(bbox=[16.15, 47.95, 16.55, 48.3], crs=CRS.WGS84)\nend_date = datetime.now()\n# Trying to take most recent scenes\nstart_date = end_date - timedelta(days=360)\n\n# Please make sure you the cdse secret is available in\n# the Credentials Manager and injection is enabled\n# or replace this with you own CDSE tokens\nCLIENT_ID = os.getenv(\"cdse_CLIENT_ID\")\nCLIENT_SECRET = os.getenv(\"cdse_CLIENT_SECRET\")\nTOKEN_URL = \"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\"\nSH_BASE_URL = \"https://sh.dataspace.copernicus.eu\"\n\n\n\n","type":"content","url":"/notebooks/data-access/cdse-tiled-data-access","position":1},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl3":"Authentication Manager"},"type":"lvl3","url":"/notebooks/data-access/cdse-tiled-data-access#authentication-manager","position":2},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl3":"Authentication Manager"},"content":"This class wraps the OAuth session to handle token refreshing automatically as it is possible to loose the session while accessing the various tiles\n\nclass CDSESessionManager:\n    \"\"\"\n    Thread-safe session manager that automatically refreshes the token \n    when a TokenExpiredError occurs.\n    \"\"\"\n    def __init__(self, client_id, client_secret):\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.token_url = TOKEN_URL\n        self.lock = threading.Lock()\n        self.session = None\n        self.authenticate()\n\n    def authenticate(self):\n        \"\"\"Create a new authenticated session\"\"\"\n        print(\"Authenticating with CDSE...\")\n        client = BackendApplicationClient(client_id=self.client_id)\n        self.session = OAuth2Session(client=client)\n        token = self.session.fetch_token(\n            token_url=self.token_url,\n            client_secret=self.client_secret,\n            include_client_id=True\n        )\n        print(f\"   Token obtained. Expires in {token.get('expires_in', 'unknown')}s\")\n        return self.session\n\n    def refresh_token(self):\n        \"\"\"Thread-safe token refresh\"\"\"\n        with self.lock:\n            # Check if token was just refreshed by another thread to avoid double-refresh\n            # (Simple heuristic: just re-auth)\n            print(\"\\nToken expired. Refreshing...\", end=\" \")\n            try:\n                self.authenticate()\n                print(\"Done.\")\n            except Exception as e:\n                print(f\"FAILED: {e}\")\n                raise\n\n    def post(self, url, json_data, retries=1):\n        \"\"\"Wrapper for session.post with auto-refresh logic\"\"\"\n        try:\n            response = self.session.post(url, json=json_data)\n            # Sometimes the server returns 401 instead of the library raising TokenExpiredError\n            if response.status_code == 401:\n                raise TokenExpiredError(\"401 Unauthorized\")\n            response.raise_for_status()\n            return response\n            \n        except TokenExpiredError:\n            if retries > 0:\n                self.refresh_token()\n                # Retry the request with new session\n                return self.post(url, json_data, retries=retries-1)\n            else:\n                raise\n\n# Initialize the Manager\nauth_manager = CDSESessionManager(CLIENT_ID, CLIENT_SECRET)\n\n\n\n\n# Collection Setup\ncoll_payload = {\n    \"type\": \"Collection\", \"stac_version\": \"1.0.0\", \"id\": STAC_COLLECTION_ID,\n    \"title\": \"Vienna Sentinel-2\", \"description\": \"Imported from CDSE\",\n    \"eodash:rasterform\": \"https://workspace-ui-public.gtif-austria.hub-otc.eox.at/api/public/share/public-4wazei3y-02/test/aducat_s2_form.json\",\n    \"license\": \"CC-BY-4.0\",\n    \"extent\": {\"spatial\": {\"bbox\": [[16.2, 48.1, 16.6, 48.3]]}, \"temporal\": {\"interval\": [[\"2020-01-01T00:00:00Z\", None]]}},\n    \"links\": []\n}\ntry: \n    requests.post(f\"{EOAPI_URL}/collections\", json=coll_payload)\nexcept: pass\n\n\n\n","type":"content","url":"/notebooks/data-access/cdse-tiled-data-access#authentication-manager","position":3},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Search in CDSE STAC"},"type":"lvl2","url":"/notebooks/data-access/cdse-tiled-data-access#search-in-cdse-stac","position":4},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Search in CDSE STAC"},"content":"\n\n# --------------------------------------------------------------------------------\n# Search for Latest Sentinel-2 Scenes\n# --------------------------------------------------------------------------------\n\ndef get_existing_dates(eoapi_url, collection_id):\n    \"\"\"\n    Fetch all unique dates (YYYY-MM-DD) currently registered in the STAC collection.\n    \"\"\"\n    existing_dates = set()\n    url = f\"{eoapi_url}/collections/{collection_id}/items\"\n    \n    # Simple pagination handling to get all items\n    params = {\"limit\": 100} \n    \n    try:\n        print(f\" Checking existing dates in collection '{collection_id}'...\")\n        while url:\n            response = requests.get(url, params=params)\n            if response.status_code == 404:\n                return set() # Collection likely doesn't exist yet\n            \n            response.raise_for_status()\n            data = response.json()\n            \n            for feature in data.get('features', []):\n                # Extract YYYY-MM-DD\n                date_str = feature['properties']['datetime'][:10]\n                existing_dates.add(date_str)\n                \n            # Check for next page link\n            url = None\n            links = data.get('links', [])\n            for link in links:\n                if link['rel'] == 'next':\n                    url = link['href']\n                    params = {} # Params usually included in next link\n                    break\n    except Exception as e:\n        print(f\" Could not fetch existing items (starting fresh?): {e}\")\n        \n    return existing_dates\n\ndef search_sentinel2_scenes(auth_manager, bbox, start_date, end_date, max_results=2):\n    \"\"\"\n    Search for Sentinel-2 scenes using CDSE OpenSearch API,\n    filtering out days that are already in the STAC collection.\n    \"\"\"\n    base_url = \"https://catalogue.dataspace.copernicus.eu/resto/api/collections/Sentinel2/search.json\"\n    \n    params = {\n        \"box\": f\"{bbox.min_x},{bbox.min_y},{bbox.max_x},{bbox.max_y}\",\n        \"startDate\": start_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"completionDate\": end_date.strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n        \"processingLevel\": \"S2MSI2A\", \n        \"cloudCover\": \"[0,5]\",\n        \"maxRecords\": 200,\n        \"sortParam\": \"startDate\",\n        \"sortOrder\": \"descending\"\n    }\n    \n    # 1. Fetch existing dates from your catalog\n    existing_dates = get_existing_dates(EOAPI_URL, STAC_COLLECTION_ID)\n    if existing_dates:\n        print(f\" Found {len(existing_dates)} days already registered. Skipping these dates.\")\n    \n    # 2. Search CDSE\n    print(\" Searching CDSE catalog...\")\n    response = requests.get(base_url, params=params)\n    response.raise_for_status()\n    \n    results = response.json()\n    features = results.get('features', [])\n    \n    # 3. Filter results\n    # Initialize seen_dates with what is already in the DB so we don't pick them again\n    seen_dates = existing_dates.copy()\n    converted_features = []\n    \n    for feature in features:\n        scene_id = feature['id']\n        # Get YYYY-MM-DD\n        scene_date = feature['properties']['startDate'][:10]\n        \n        # SKIP IF DATE ALREADY EXISTS (in DB or earlier in this loop)\n        if scene_date in seen_dates:\n            continue\n            \n        # Mark this date as seen so we don't pick another scene from the same day\n        seen_dates.add(scene_date)\n        \n        converted_features.append({\n            'id': scene_id,\n            'properties': {\n                'datetime': feature['properties']['startDate'],\n                'eo:cloud_cover': feature['properties'].get('cloudCover', 0)\n            },\n            'geometry': feature.get('geometry'),\n            'bbox': feature.get('bbox')\n        })\n        \n        if len(converted_features) >= max_results:\n            break\n    \n    return converted_features\n\n# Run the search, here we configure the area and time of interest as well as how many scenes to fetch\nscenes = search_sentinel2_scenes(auth_manager, bbox, start_date, end_date, 5)\n\nprint(f\"\\nSelected {len(scenes)} NEW scenes for processing:\")\nfor i, scene in enumerate(scenes, 1):\n    print(f\"{i}. ID: {scene['id']}\")\n    print(f\"   Date: {scene['properties']['datetime']}\")\n    print(f\"   Cloud Cover: {scene['properties'].get('eo:cloud_cover', 'N/A')}%\\n\")\n\n\n\n","type":"content","url":"/notebooks/data-access/cdse-tiled-data-access#search-in-cdse-stac","position":5},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Data conversion"},"type":"lvl2","url":"/notebooks/data-access/cdse-tiled-data-access#data-conversion","position":6},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Data conversion"},"content":"\n\n\n# --- Configuration ---\nTARGET_RES = 10 \nMAX_PIXELS = 2000 \nMAX_WORKERS = 2 \nMAX_RETRIES = 3\nRETRY_DELAY = 2\n\n@dataclass\nclass SimpleBBox:\n    min_x: float; min_y: float; max_x: float; max_y: float\n\ndef split_bbox_into_tiles(bbox):\n    w_deg, h_deg = bbox.max_x - bbox.min_x, bbox.max_y - bbox.min_y\n    center_lat = (bbox.min_y + bbox.max_y) / 2\n    m_lat, m_lon = 111320, 111320 * math.cos(math.radians(center_lat))\n    tot_w_px, tot_h_px = int((w_deg * m_lon) / TARGET_RES), int((h_deg * m_lat) / TARGET_RES)\n    cols, rows = math.ceil(tot_w_px / MAX_PIXELS), math.ceil(tot_h_px / MAX_PIXELS)\n    \n    tiles = []\n    print(f\"    Tiling: {cols}x{rows} grid\")\n    for r in range(rows):\n        for c in range(cols):\n            tiles.append({\n                \"bbox\": SimpleBBox(\n                    bbox.min_x + (c * (w_deg/cols)), \n                    bbox.min_y + (r * (h_deg/rows)), \n                    bbox.min_x + ((c+1) * (w_deg/cols)), \n                    bbox.min_y + ((r+1) * (h_deg/rows))\n                ),\n                \"size\": (int(tot_w_px/cols), int(tot_h_px/rows)),\n                \"index\": (r, c)\n            })\n    return tiles\n\ndef convert_to_cog(path, arr, transform, crs):\n    prof = cog_profiles.get(\"deflate\").copy()\n    prof.update({\n        \"BIGTIFF\": \"IF_NEEDED\",\n        \"blockxsize\": 512,\n        \"blockysize\": 512,\n        \"photometric\": \"MINISBLACK\"\n    })\n    with MemoryFile() as mem:\n        with mem.open(driver=\"GTiff\", dtype=str(arr.dtype), count=arr.shape[0], \n                      height=arr.shape[1], width=arr.shape[2], crs=crs, \n                      transform=transform, photometric=\"MINISBLACK\") as dst:\n            dst.write(arr)\n            dst.descriptions = tuple(ALL_BANDS)\n            dst.build_overviews([2,4,8,16], Resampling.nearest)\n            dst.update_tags(ns='rio_overview', resampling='nearest')\n        with mem.open() as src:\n            cog_translate(src, path, prof, in_memory=True, quiet=True)\n    print(f\"   COG Saved: {path}\")\n\ndef download_single_tile(auth_mgr, tile_data, scene_id, scene_datetime):\n    \"\"\"Worker function with RETRY logic\"\"\"\n    dt = datetime.fromisoformat(scene_datetime.replace('Z', '+00:00'))\n    \n    # ... (Evalscript needed for data extraction) ...\n    evalscript = \"\"\"\n    //VERSION=3\n    function setup() {\n        return {\n            input: [{ bands: \"\"\" + json.dumps(ALL_BANDS) + \"\"\", units: \"DN\" }],\n            output: { bands: \"\"\" + str(len(ALL_BANDS)) + \"\"\", sampleType: \"UINT16\" }\n        };\n    }\n    function evaluatePixel(sample) {\n        return [\"\"\" + \", \".join([f\"sample.{b}\" for b in ALL_BANDS]) + \"\"\"];\n    }\n    \"\"\"\n    \n    t_bbox = tile_data['bbox']\n    payload = {\n        \"input\": {\n            \"bounds\": {\n                \"bbox\": [t_bbox.min_x, t_bbox.min_y, t_bbox.max_x, t_bbox.max_y],\n                \"properties\": {\"crs\": \"http://www.opengis.net/def/crs/EPSG/0/4326\"}\n            },\n            \"data\": [{\n                \"type\": \"sentinel-2-l2a\",\n                \"dataFilter\": {\n                    \"timeRange\": {\n                        \"from\": (dt - timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\"),\n                        \"to\": (dt + timedelta(minutes=30)).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n                    }\n                }\n            }]\n        },\n        \"output\": {\n            \"width\": tile_data['size'][0], \"height\": tile_data['size'][1],\n            \"responses\": [{\"identifier\": \"default\", \"format\": {\"type\": \"image/tiff\"}}]\n        },\n        \"evalscript\": evalscript\n    }\n    \n    # --- Retry Loop ---\n    for attempt in range(1, MAX_RETRIES + 1):\n        try:\n            response = auth_mgr.post(f\"{SH_BASE_URL}/api/v1/process\", payload)\n            # Assuming auth_mgr raises an exception on non-200. \n            # If not, check response.status_code here.\n            \n            # Simple validation to ensure we actually got data\n            if response.content and len(response.content) > 100:\n                return tile_data['index'], response.content\n            else:\n                raise Exception(\"Empty response received\")\n\n        except Exception as e:\n            # If this was the last attempt, fail\n            if attempt == MAX_RETRIES:\n                print(f\"      Tile {tile_data['index']} failed permanently: {e}\")\n                return tile_data['index'], None\n            \n            # Otherwise, wait and retry\n            sleep_time = RETRY_DELAY * attempt\n            print(f\"      Tile {tile_data['index']} failed (Attempt {attempt}/{MAX_RETRIES}). Retrying in {sleep_time}s...\")\n            time.sleep(sleep_time)\n\n    return tile_data['index'], None\n\n# --- Main Loop ---\nprocessed_scenes = []\ntiles = split_bbox_into_tiles(bbox) \n\nfor scene in scenes:\n    s_id, s_dt = scene['id'], scene['properties']['datetime']\n    cog_path = output_dir / f\"{s_dt[:10]}_{s_id}_cog.tif\"\n    print(f\"\\nProcessing {s_id}...\")\n\n    if cog_path.exists():\n        print(f\"   File exists. Skipping download.\")\n        processed_scenes.append({\"id\": s_id, \"path\": cog_path, \"props\": scene['properties'], \"geom\": scene['geometry']})\n        continue\n\n    print(f\"   Downloading {len(tiles)} tiles (Parallel)...\")\n    tile_results = {}\n    \n    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n        futures = {executor.submit(download_single_tile, auth_manager, t, s_id, s_dt): t for t in tiles}\n        \n        for f in concurrent.futures.as_completed(futures):\n            idx, content = f.result()\n            # Only store if content is valid\n            if content: \n                tile_results[idx] = content\n    \n    # --- Completeness Check ---\n    # We compare the number of valid results to the number of expected tiles.\n    if len(tile_results) != len(tiles):\n        missing_count = len(tiles) - len(tile_results)\n        print(f\"   Scene failed. Missing {missing_count} tiles. Discarding scene.\")\n        # We 'continue' here to move to the next scene without stitching\n        continue\n\n    print(\"   Stitching & Converting...\")\n    m_files, m_dsets = [], []\n    try:\n        for k in sorted(tile_results.keys()):\n            mf = MemoryFile(tile_results[k])\n            m_files.append(mf); m_dsets.append(mf.open())\n        \n        mosaic, trans = merge(m_dsets)\n        convert_to_cog(str(cog_path), mosaic, trans, m_dsets[0].crs)\n        \n        if cog_path.exists():\n            processed_scenes.append({\"id\": s_id, \"path\": cog_path, \"props\": scene['properties'], \"geom\": scene['geometry']})\n    except Exception as e:\n        print(f\"   Stitching failed: {e}\")\n    finally:\n        for d in m_dsets: d.close()\n        for m in m_files: m.close()\n\n\n\n","type":"content","url":"/notebooks/data-access/cdse-tiled-data-access#data-conversion","position":7},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Register in eoAPI"},"type":"lvl2","url":"/notebooks/data-access/cdse-tiled-data-access#register-in-eoapi","position":8},{"hierarchy":{"lvl1":"CDSE Tiled Data Access","lvl2":"Register in eoAPI"},"content":"If wanted this example continues explaining how the retrieved data can be registered into an eoAPI instance\n\ndef create_and_register(scene, eoapi_url, collection_id):\n    s_id, props = scene['id'], scene['props']\n    dt = datetime.fromisoformat(props['datetime'].replace('Z', '+00:00'))\n    cog_url = f\"{S3_BUCKET}/{S3_PREFIX}/{scene['path'].name}\"\n    \n    # Geometry fallback\n    geom = scene['geom']\n    if not geom: \n        geom = {\"type\": \"Polygon\", \"coordinates\": [[ [bbox.min_x, bbox.min_y], [bbox.max_x, bbox.min_y], [bbox.max_x, bbox.max_y], [bbox.min_x, bbox.max_y], [bbox.min_x, bbox.min_y] ]]}\n        \n    # Get BBox from geometry for STAC Item\n    coords = geom['coordinates'][0]\n    xs, ys = [c[0] for c in coords], [c[1] for c in coords]\n    ibbox = [min(xs), min(ys), max(xs), max(ys)]\n\n    item = pystac.Item(id=s_id, geometry=geom, bbox=ibbox, datetime=dt, \n                       properties={\"eo:cloud_cover\": props.get('eo:cloud_cover', 0)}, collection=collection_id)\n    \n    item.add_asset(\"cog\", pystac.Asset(href=cog_url, media_type=\"image/tiff; application=geotiff; profile=cloud-optimized\", roles=[\"data\", \"visual\"]))\n    \n    # Add XYZ Preview Link\n    eoapi_endpoint = os.getenv(\"workspace_RASTER_ENDPOINT\")\n    xyz = f\"{eoapi_endpoint}/collections/{collection_id}/items/{s_id}/tiles/WebMercatorQuad/{{z}}/{{x}}/{{y}}@1x?assets=cog&asset_bidx=cog|4,3,2&rescale=0,3000\"\n    item.add_link(pystac.Link(rel=\"xyz\", target=xyz, media_type=\"application/json\"))\n\n    res = requests.post(f\"{eoapi_url}/collections/{collection_id}/items\", json=item.to_dict())\n    if res.status_code in [200, 201]: print(f\" Registered {s_id}\")\n    else: print(f\" Failed {s_id}: {res.text}\")\n\nprint(f\"Registration...\")\nfor s in processed_scenes:\n    create_and_register(s, EOAPI_URL, STAC_COLLECTION_ID)\n\n\n\n# --------------------------------------------------------------------------------\n# Finally we update Collection Temporal Extent\n# --------------------------------------------------------------------------------\nimport requests\n\ndef update_collection_extent(eoapi_url, collection_id):\n    print(f\"Updating temporal extent for collection: '{collection_id}'...\")\n\n    # 1. Fetch all item datetimes to determine the full range\n    all_datetimes = []\n    items_url = f\"{eoapi_url}/collections/{collection_id}/items\"\n    params = {\"limit\": 100}\n\n    try:\n        # Loop through pages to get every single item's date\n        while items_url:\n            r = requests.get(items_url, params=params)\n            r.raise_for_status()\n            data = r.json()\n            \n            for feature in data.get('features', []):\n                all_datetimes.append(feature['properties']['datetime'])\n            \n            # Handle Pagination (Follow 'next' link)\n            items_url = None\n            for link in data.get('links', []):\n                if link['rel'] == 'next':\n                    items_url = link['href']\n                    params = {} # Parameters are usually included in the 'next' href\n                    break\n\n        if not all_datetimes:\n            print(\" No items found in collection. Skipping extent update.\")\n            return\n\n        # 2. Calculate new Min/Max\n        min_dt = min(all_datetimes)\n        max_dt = max(all_datetimes)\n        \n        print(f\"   Found {len(all_datetimes)} items.\")\n        print(f\"   Calculated Interval: {min_dt}  <-->  {max_dt}\")\n\n        # 3. Fetch the current Collection definition\n        coll_url = f\"{eoapi_url}/collections/{collection_id}\"\n        coll_resp = requests.get(coll_url)\n        coll_resp.raise_for_status()\n        coll_data = coll_resp.json()\n        \n        # 4. Update the extent in the JSON\n        # STAC Spec: extent.temporal.interval is a list of lists [[start, end]]\n        coll_data['extent']['temporal']['interval'] = [[min_dt, max_dt]]\n        \n        # 5. Send PUT request to save changes\n        update_resp = requests.put(coll_url, json=coll_data)\n        \n        if update_resp.status_code in [200, 204]:\n            print(f\" Success! Collection '{collection_id}' temporal extent updated.\")\n        else:\n            print(f\" Failed to update collection. Server returned: {update_resp.status_code}\")\n            print(update_resp.text)\n\n    except Exception as e:\n        print(f\" Error updating extent: {e}\")\n\n# Run the update\nupdate_collection_extent(EOAPI_URL, STAC_COLLECTION_ID)\n\n","type":"content","url":"/notebooks/data-access/cdse-tiled-data-access#register-in-eoapi","position":9},{"hierarchy":{"lvl1":"eoAPI registration example"},"type":"lvl1","url":"/notebooks/eoapi/eoapi-registration","position":0},{"hierarchy":{"lvl1":"eoAPI registration example"},"content":"This notebook demonstrates how to register data on the workspace eoAPI instance. The main step is creation of STAC metadata information to pass it to the eoAPI STAC endpoint.\nIn order to show the full flow this notebooks also fetches some sample data and does following steps:\n\nDownload: Download file to mounted bucket path.\n\nCollection creation: Create the STAC collection the file will be registered to.\n\nMetadata extraction: Extract as much information as possible from source file to create STAC item.\n\nIngest: Post the resulting STAC ttem to the eoAPI STAC endpoint.\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration","position":1},{"hierarchy":{"lvl1":"eoAPI registration example","lvl3":"Environment Checks"},"type":"lvl3","url":"/notebooks/eoapi/eoapi-registration#environment-checks","position":2},{"hierarchy":{"lvl1":"eoAPI registration example","lvl3":"Environment Checks"},"content":"\n\n# First some checks to see the environment is as expected\nimport sys\nimport os\nimport importlib.util\n\n# List the packages to check\nrequired = [\"pystac\", \"rio_stac\", \"pystac_client\", \"requests\", \"rasterio\"]\nmissing = [p for p in required if importlib.util.find_spec(p.replace('-','_')) is None]\n\nif missing:\n    print(f\" ERROR: Missing packages: {', '.join(missing)}\")\n    print(\"It looks like the wrong Conda kernel is active. Please switch kernels and try again.\")\n    sys.exit(\"Execution stopped: Missing dependencies.\")\n\n# If check passes, proceed with imports\nfrom datetime import datetime, timezone\nfrom pystac import Collection, Extent, SpatialExtent, TemporalExtent, Link\nfrom rio_stac import create_stac_item\nimport rasterio\nimport requests\n\nprint(\" Environment ready.\")\n\n\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#environment-checks","position":3},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"Configuration"},"type":"lvl2","url":"/notebooks/eoapi/eoapi-registration#configuration","position":4},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"Configuration"},"content":"If using your own file adjust FILE_PATH to match its location and remove the download step.\n\nSTAC_API_URL = \"http://eoapi-rw-stac:8080\" \nCOLLECTION_ID = \"demo-collection\"\nCOG_SOURCE_URL = \"https://sentinel-cogs.s3.us-west-2.amazonaws.com/sentinel-s2-l2a-cogs/36/Q/WD/2020/7/S2A_36QWD_20200701_0_L2A/TCI.tif\"\nFILE_PATH = \"./bucket/demo_data/TCI.tif\"\n\nos.makedirs(os.path.dirname(FILE_PATH), exist_ok=True)\n\n\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#configuration","position":5},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"1. Download"},"type":"lvl2","url":"/notebooks/eoapi/eoapi-registration#id-1-download","position":6},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"1. Download"},"content":"\n\n# First we download the example file and put it into the mounted bucket\n# You can also use the File Browser to upload your data\n# (should be used for files smaller then 1 GB)\nif os.path.exists(FILE_PATH):\n    print(f\"File exists at {FILE_PATH}. Skipping.\")\nelse:\n    print(f\"Downloading to {FILE_PATH}...\")\n    with requests.get(COG_SOURCE_URL, stream=True) as r:\n        r.raise_for_status()\n        with open(FILE_PATH, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n\n\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#id-1-download","position":7},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"2. Collection creation"},"type":"lvl2","url":"/notebooks/eoapi/eoapi-registration#id-2-collection-creation","position":8},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"2. Collection creation"},"content":"\n\n# Then we create the collection (only if it does not already exist)\ncheck_url = f\"{STAC_API_URL}/collections/{COLLECTION_ID}\"\nresp = requests.get(check_url)\nif resp.status_code == 200:\n    print(f\"Collection '{COLLECTION_ID}' exists.\")\nelse:\n    print(f\"Creating collection '{COLLECTION_ID}'...\")\n    coll = Collection(\n        id=COLLECTION_ID,\n        title=\"Demo collection\",\n        description=\"Demo COG collection\",\n        extent=Extent(SpatialExtent([[-180, -90, 180, 90]]), TemporalExtent([[datetime(2020,1,1,tzinfo=timezone.utc), None]])),\n        license=\"CC-BY-4.0\"\n    )\n    # Sending a POST request with the Collection json to create the collection\n    requests.post(f\"{STAC_API_URL}/collections\", json=coll.to_dict()).raise_for_status()\n\n\n\n# If we wanted to delete the collection we could use following command\n# requests.delete(f\"{STAC_API_URL}/collections/{COLLECTION_ID}\")\n\n\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#id-2-collection-creation","position":9},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"3. Metadata Extraction"},"type":"lvl2","url":"/notebooks/eoapi/eoapi-registration#id-3-metadata-extraction","position":10},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"3. Metadata Extraction"},"content":"\n\n# We need a STAC item definition in order to register it into the collection\n# see https://github.com/radiantearth/stac-spec/blob/master/item-spec/item-spec.md for further details\n\n# rio-stac provides a useful helper tool create_stac_item\n# to extract metadata from the raster file and create a STAC item from it\n\n# We need to define the connected bucket to this workspace\nBUCKET = os.getenv(\"workspace_BUCKET\")\nROOT_FILE_PATH = FILE_PATH.replace(\"./bucket\", \"\")\n# cloud location eoAPI TiTiler will access the data assets from\nS3_URI = f\"s3://{BUCKET}{ROOT_FILE_PATH}\"\n\nitem = create_stac_item(\n    source=FILE_PATH,\n    asset_href=S3_URI,\n    id=os.path.basename(ROOT_FILE_PATH).split('.')[0],\n    collection=COLLECTION_ID,\n    asset_name=\"data\"\n)\n\n# Add preview link to allow rendering data in eoAPI GUI\neoapi_endpoint = os.getenv(\"workspace_RASTER_ENDPOINT\")\nxyz = f\"{eoapi_endpoint}/collections/{COLLECTION_ID}/items/{item.id}/tiles/WebMercatorQuad/{{z}}/{{x}}/{{y}}?assets=data\"\nitem.add_link(Link(rel=\"xyz\", target=xyz, media_type=\"application/json\"))\n\n\n\n\n# We can print the item by uncommenting if we want to have a look\n# item.to_dict()\n\n\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#id-3-metadata-extraction","position":11},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"4. Ingestion"},"type":"lvl2","url":"/notebooks/eoapi/eoapi-registration#id-4-ingestion","position":12},{"hierarchy":{"lvl1":"eoAPI registration example","lvl2":"4. Ingestion"},"content":"\n\n# Now that the item was created we can pass it to eoAPI for ingestion\n\nurl = f\"{STAC_API_URL}/collections/{COLLECTION_ID}/items\"\nresponse = requests.post(url, json=item.to_dict())\n\nif response.status_code in [200, 201]:\n    print(\"Successfully posted STAC Item.\")\nelse:\n    print(f\"Error {response.status_code}: {response.text}\")\n\n\n\n# Update temporal and spatial extent based on ingested datasets\nimport requests\n\ndef update_stac(url, cid):\n    # 1. Fetch all items (limited to 1000 for brevity)\n    items = requests.get(f\"{url}/collections/{cid}/items?limit=1000\").json()['features']\n    \n    # 2. Extract and calculate (flatten coordinates and collect dates)\n    coords = [pt for f in items for pt in f['geometry']['coordinates'][0]]\n    dts = [f['properties']['datetime'] for f in items]\n    \n    # 3. Patch the collection\n    ext = {\n        \"spatial\": {\"bbox\": [[min(c[0] for c in coords), min(c[1] for c in coords), \n                             max(c[0] for c in coords), max(c[1] for c in coords)]]},\n        \"temporal\": {\"interval\": [[min(dts), max(dts)]]}\n    }\n    \n    r = requests.patch(f\"{url}/collections/{cid}\", json={\"extent\": ext})\n    print(\"✅ Updated\" if r.ok else f\"❌ Error: {r.text}\")\n\nupdate_stac(STAC_API_URL, COLLECTION_ID)\n\n\n\nfrom IPython.display import Image, display\n\nurl = f\"{eoapi_endpoint}/collections/demo-collection/items/TCI/preview?assets=data\"\ndisplay(Image(url=url, width=500))\n\n","type":"content","url":"/notebooks/eoapi/eoapi-registration#id-4-ingestion","position":13},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example"},"type":"lvl1","url":"/notebooks/pangeo-dask-notebooks/example-wildfires","position":0},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example"},"content":"","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires","position":1},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Create a burn severity maps using Sentinel-2 Cloud-Optimised Dataset"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#create-a-burn-severity-maps-using-sentinel-2-cloud-optimised-dataset","position":2},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Create a burn severity maps using Sentinel-2 Cloud-Optimised Dataset"},"content":"","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#create-a-burn-severity-maps-using-sentinel-2-cloud-optimised-dataset","position":3},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Context"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#context","position":4},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Context"},"content":"","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#context","position":5},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Purpose","lvl3":"Context"},"type":"lvl4","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#purpose","position":6},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Purpose","lvl3":"Context"},"content":"Demonstrate how to fetch satellite Sentinel-2 data to generate burn severity maps for the assessment of the areas affected by wildfires.","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#purpose","position":7},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Methodology approach","lvl3":"Context"},"type":"lvl4","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#methodology-approach","position":8},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Methodology approach","lvl3":"Context"},"content":"Access Sentinel-2 L2A cloud optimised dataset through STAC\n\nCompute the Normalised Burn Ratio (NBR) index to highlight burned areas\n\nClassify burn severity","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#methodology-approach","position":9},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Highlights","lvl3":"Context"},"type":"lvl4","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#highlights","position":10},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl4":"Highlights","lvl3":"Context"},"content":"The NBR index uses near-infrared (NIR) and shortwave-infrared (SWIR) wavelengths.","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#highlights","position":11},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl5":"Related publications","lvl4":"Highlights","lvl3":"Context"},"type":"lvl5","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#related-publications","position":12},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl5":"Related publications","lvl4":"Highlights","lvl3":"Context"},"content":"https://​www​.sciencedirect​.com​/science​/article​/pii​/S1470160X22004708​#f0035\n\nhttps://github.com/yobimania/dea-notebooks/blob/e0ca59f437395f7c9becca74badcf8c49da6ee90/Fire Analysis Compiled Scripts (Gadi)/dNBR_full.py\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#related-publications","position":13},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Dask use ‘Client’ as well, thus pystac_client  is renamed"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#dask-use-client-as-well-thus-pystac-client-is-renamed","position":14},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Dask use ‘Client’ as well, thus pystac_client  is renamed"},"content":"\n\nimport os\n#import dask.distributed\n\nfrom pystac_client import Client as pystac_client\nfrom odc.stac import configure_rio, stac_load\nimport matplotlib.pyplot as plt\nimport cartopy.crs as ccrs\nimport cartopy.feature as cfeature\n\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#dask-use-client-as-well-thus-pystac-client-is-renamed","position":15},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl2":"Get Dask Client"},"type":"lvl2","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#get-dask-client","position":16},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl2":"Get Dask Client"},"content":"\n\nfrom dask_gateway import Gateway\ngateway = Gateway()\n\n\n\ncluster_options = gateway.cluster_options()\ncluster_options\n\n\n\nimport sys\ncluster_options.env_path = sys.prefix\n\n\n\ncluster = gateway.new_cluster(cluster_options=cluster_options)\n# cluster = gateway.new_cluster(cluster_options=cluster_options)\ncluster.scale(2)\nclient = cluster.get_client()\nclient\n\n\n\ncluster = gateway.connect(gateway.list_clusters()[0].name)\n\n# cluster.shutdown()\ngateway.list_clusters()\nclient = cluster.get_client()\nclient\n\n\n\n# import os\n# client.run(os.getenv, 'CONDA_PREFIX')\n\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#get-dask-client","position":17},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Set project structure","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#set-project-structure","position":18},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Set project structure","lvl2":"Get Dask Client"},"content":"The cell below creates a separate folder to save the notebook outputs. This facilitates the reader to inspect inputs/outputs stored within a defined destination folder. Change <replace-by-notebook-filename> with your notebook identifier.\n\nnotebook_folder = './wildfires-foss4g'\nif not os.path.exists(notebook_folder):\n    os.makedirs(notebook_folder)\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#set-project-structure","position":19},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Load data","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#load-data","position":20},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Load data","lvl2":"Get Dask Client"},"content":"Load full dataset from original or mirror sources. If the license of the dataset permits, we suggest creating sample data (preprocessed) for the notebook stored in a data repository e.g. Zenodo.\n\nindex_name = 'NBR'\n\nbandnames_dict = {\n    'nir': 'nir',\n    'swir22': 'swir22'\n}\n\nkm2deg = 1.0 / 111\nx, y = (23.9983519, 37.7351433)  # Center point of a query\nr = 4 * km2deg  \nbbox = (x - r, y - r, x + r, y + r)\nzoom = 1\n\ncrs = \"epsg:3857\"  # projection on which the data will be projected\n\n# Normalised Burn Ratio, Lopez Garcia 1991\n# index_dict = {'NBR': lambda ds: (ds.nir - ds.swir22) / (ds.nir + ds.swir22)}\n\ndef calc_nbr(ds):\n    return (ds.nir - ds.swir22) / (ds.nir + ds.swir22)\n\nindex_dict = {'NBR': calc_nbr}\nindex_dict\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#load-data","position":21},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Show location on a map","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#show-location-on-a-map","position":22},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Show location on a map","lvl2":"Get Dask Client"},"content":"\n\nfig = plt.figure(1, figsize=[15, 10])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.set_extent([15.5, 27.5, 36, 41], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\nax.add_feature(cfeature.OCEAN)\nax.add_feature(cfeature.LAND)\n\nplt.plot(x, y,\n         color='magenta', markersize=15, marker='s',\n         transform=ccrs.PlateCarree(),\n         )\n\n# One way to customize your title\nplt.title(\"Map of the area of interest\", fontsize=18)\n\n\n\n\n\n## Open Catalog and get data\n\n## DASK UPDATE***\n    \n# time_range = \"2017-08-17/2022-08-20\"\n\n# catalog = pystac_client.open(\"https://catalog.osc.earthcode.eox.at\")\n# query1 = catalog.search(\n#     datetime=time_range, limit=100,\n#     bbox=bbox,\n# )\n# items = list(query1.get_items())\n# items\n\n\n\n# Open a catalog\ncatalog = pystac_client.open(\"https://earth-search.aws.element84.com/v1\")\n\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#show-location-on-a-map","position":23},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Increase computation and try Dask","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#increase-computation-and-try-dask","position":24},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Increase computation and try Dask","lvl2":"Get Dask Client"},"content":"\n\nzoom=1/2\nchunk={\"y\":100}\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#increase-computation-and-try-dask","position":25},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Search and get data before the fire","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#search-and-get-data-before-the-fire","position":26},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Search and get data before the fire","lvl2":"Get Dask Client"},"content":"time_range contains the period over which data will be searched;\n\nquery contains additional requirements e.g. get data only when the cloud cover is low (< 0.5)\n\n# prefire data\ntime_range = \"2021-08-10/2021-08-16\"\n\nquery1 = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}},\n)\n\nitems = list(query1.get_items())\nprint(f\"Found: {len(items):d} datasets\")\n\nitems_pre = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n\nprefire_ds = stac_load(\n    [items_pre],\n    bands=(\"nir\", \"swir22\"),\n    crs=crs,\n    resolution=  10*zoom,\n    chunks=chunk,  # <-- use Dask\n    groupby=\"datetime\",\n    bbox=bbox,\n)\nprefire_ds\n\n\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#search-and-get-data-before-the-fire","position":27},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Search and get data after the fire","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#search-and-get-data-after-the-fire","position":28},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Search and get data after the fire","lvl2":"Get Dask Client"},"content":"\n\n##postfire\ntime_range = \"2021-08-17/2021-08-20\"\n\nquery2 = catalog.search(\n    collections=[\"sentinel-2-l2a\"], datetime=time_range, limit=100,\n    bbox=bbox, query={\"eo:cloud_cover\": {\"lt\": 0.5}},\n)\n\nitems = list(query2.get_items())\nprint(f\"Found: {len(items):d} datasets\")\n\nitems_post = min(items, key=lambda item: item.properties[\"eo:cloud_cover\"])\n\npostfire_ds = stac_load(\n    [items_post],\n    bands=(\"nir\", \"swir22\"),\n    crs=crs,\n    resolution=10 * zoom,\n    chunks=chunk,  # <-- use Dask\n    groupby=\"datetime\",\n    bbox=bbox,\n)\npostfire_ds\n\n\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#search-and-get-data-after-the-fire","position":29},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Methodology","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#methodology","position":30},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Methodology","lvl2":"Get Dask Client"},"content":"Add code demonstrating the methodology.\n\n# Rename bands in dataset to use simple names \nbands_to_rename = {\n    a: b for a, b in bandnames_dict.items() if a in prefire_ds.variables\n}\n\n# prefire\nprefire_ds[index_name] = index_dict[index_name](prefire_ds.rename(bands_to_rename) / 10000.0)\n\n# postfire\npostfire_ds[index_name] = index_dict[index_name](postfire_ds.rename(bands_to_rename) / 10000.0)\n\n\n\n\n# calculate delta NBR\nprefire_burnratio = prefire_ds.NBR.isel(time=0)\npostfire_burnratio = postfire_ds.NBR.isel(time=0)\n\ndelta_NBR = prefire_burnratio - postfire_burnratio\n\ndnbr_dataset = delta_NBR.to_dataset(name='delta_NBR')\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#methodology","position":31},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Outputs","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#outputs","position":32},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Outputs","lvl2":"Get Dask Client"},"content":"Provide a brief inspection of the methodology outputs and their interpretation\n\ndnbr_dataset\ndelta_NBR\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#outputs","position":33},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Zoom on the area","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#zoom-on-the-area","position":34},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Zoom on the area","lvl2":"Get Dask Client"},"content":"DASK ‘lazy’ computation starts here.  Watchout the Task Stream and Worker CPU usage for following 3 plots!!\n\nfig = plt.figure(1, figsize=[7, 10])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\nprefire_burnratio.plot(ax=ax, transform=ccrs.epsg(prefire_burnratio.spatial_ref.values), cmap='RdBu_r',\n                       cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title( pd.to_datetime(prefire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)\n\n\n\nfig = plt.figure(1, figsize=[7, 9])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\n#ax.set_extent([-180, 180, -70, 70], crs=ccrs.PlateCarree()) # lon1 lon2 lat1 lat2\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\npostfire_burnratio.plot(ax=ax, transform=ccrs.epsg(postfire_burnratio.spatial_ref.values), cmap='RdBu_r',\n                        cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title( pd.to_datetime(postfire_burnratio.time.values.item()).strftime(\"%d %B %Y\"), fontsize=18)\n\n\n\nfig = plt.figure(1, figsize=[7, 10])\n\n# We're using cartopy and are plotting in PlateCarree projection \n# (see documentation on cartopy)\nax = plt.subplot(1, 1, 1, projection=ccrs.PlateCarree())\nax.coastlines(resolution='10m')\nax.gridlines(draw_labels=True)\n\n# We need to project our data to the new Orthographic projection and for this we use `transform`.\n# we set the original data projection in transform (here Mercator)\ndnbr_dataset.delta_NBR.plot(ax=ax, transform=ccrs.epsg(dnbr_dataset.delta_NBR.spatial_ref.values), cmap='RdBu_r',\n                            cbar_kwargs={'orientation':'horizontal','shrink':0.95})\n\n# One way to customize your title\nplt.title( \"Delta NBR\", fontsize=18)\n\n\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#zoom-on-the-area","position":35},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Save Dataset as .zarr","lvl2":"Get Dask Client"},"type":"lvl3","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#save-dataset-as-zarr","position":36},{"hierarchy":{"lvl1":"Scale your processes with Dask - hands on example","lvl3":"Save Dataset as .zarr","lvl2":"Get Dask Client"},"content":"\n\n# Define the output path within your notebook folder\noutput_path = os.path.join(notebook_folder, \"dnbr_dataset.zarr\")\n\n# Because your dataset is lazy (using Dask), it may not have computed all values before the write is attempted. Try computing the dataset explicitly before saving:\n# This ensures that all lazy operations are materialized in memory, which can sometimes resolve issues with missing metadata.\ndnbr_dataset = dnbr_dataset.compute()\n\n# save\ndnbr_dataset.to_zarr(output_path, mode=\"w\")\n\n\n\n\n\n\n# import os\n# import xarray as xr\n\n# # Define the folder where your Zarr store is saved\n# notebook_folder = './wildfires-foss4g'\n# zarr_path = os.path.join(notebook_folder, \"dnbr_dataset.zarr\")\n\n# # Use the same chunking scheme as before\n# chunk = {\"y\": 100}\n\n# # Open the Zarr store as a Dask-backed xarray.Dataset\n# dnbr_ds = xr.open_zarr(zarr_path, chunks=chunk)\n\n# dnbr_ds\n\n","type":"content","url":"/notebooks/pangeo-dask-notebooks/example-wildfires#save-dataset-as-zarr","position":37}]}